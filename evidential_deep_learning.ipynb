{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zain-mahfoud94/Python-Uni/blob/main/evidential_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd9377ef-9a8b-4d9f-9b5c-3d7efbe9b8aa",
      "metadata": {
        "id": "cd9377ef-9a8b-4d9f-9b5c-3d7efbe9b8aa"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%matplotlib inline\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "692671b0-6713-4639-8921-e2f88596a332",
      "metadata": {
        "id": "692671b0-6713-4639-8921-e2f88596a332"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "import multiprocessing\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "sys.path.append(os.path.join(os.path.abspath('..'), 'src'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70dff956-ac48-4cd5-a57c-b03cd8ebfab6",
      "metadata": {
        "id": "70dff956-ac48-4cd5-a57c-b03cd8ebfab6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e521076-26ad-4511-b874-854000b49df9",
      "metadata": {
        "id": "0e521076-26ad-4511-b874-854000b49df9"
      },
      "outputs": [],
      "source": [
        "from data_split import return_datasets\n",
        "from time_series_loader import TimeSeriesData"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1cf78c6-046f-4472-ae98-6a3d1f71b45d",
      "metadata": {
        "id": "d1cf78c6-046f-4472-ae98-6a3d1f71b45d"
      },
      "source": [
        "# Ressources variables and PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e93ca39-348c-48c3-9299-d0db38d5c921",
      "metadata": {
        "id": "2e93ca39-348c-48c3-9299-d0db38d5c921"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b78131a8-b2f2-404c-b3c4-240179d5e6fc",
      "metadata": {
        "id": "b78131a8-b2f2-404c-b3c4-240179d5e6fc",
        "outputId": "2ba83c95-2134-4b65-af03-2d4be88fb756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on:  cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Training on: ', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a07a30d-b7a7-479a-a872-64d2453363c9",
      "metadata": {
        "id": "7a07a30d-b7a7-479a-a872-64d2453363c9",
        "outputId": "3c3e845c-8e74-4201-fdc0-fea27aa66e9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using  10  cpu cores\n"
          ]
        }
      ],
      "source": [
        "SLURM_CPUS_PER_TASK = os.getenv(\"SLURM_CPUS_PER_TASK\")\n",
        "\n",
        "if SLURM_CPUS_PER_TASK is None:\n",
        "    n_cpu = multiprocessing.cpu_count()\n",
        "    SLURM_CPUS_PER_TASK = f'{n_cpu}'\n",
        "else:\n",
        "    n_cpu = int(SLURM_CPUS_PER_TASK)\n",
        "\n",
        "\n",
        "os.environ[\"OMP_NUM_THREADS\"] = SLURM_CPUS_PER_TASK\n",
        "os.environ[\"MKL_NUM_THREADS\"] = SLURM_CPUS_PER_TASK\n",
        "os.environ[\"NUMEXPR_NUM_THREADS\"] = SLURM_CPUS_PER_TASK\n",
        "\n",
        "print('Using ', SLURM_CPUS_PER_TASK, ' cpu cores')\n",
        "\n",
        "torch.set_num_threads(int(SLURM_CPUS_PER_TASK))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "638ae837-6539-443e-bf1c-2260dab2f398",
      "metadata": {
        "id": "638ae837-6539-443e-bf1c-2260dab2f398"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12f23375-a444-4469-8222-249b4282524b",
      "metadata": {
        "id": "12f23375-a444-4469-8222-249b4282524b"
      },
      "outputs": [],
      "source": [
        "data_path = os.path.join(os.path.abspath('../../../../../..'), 'datasets', 'DLL', 'aimee', 'processed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84c2035b-5245-4e97-aeee-6b0756ebdc50",
      "metadata": {
        "id": "84c2035b-5245-4e97-aeee-6b0756ebdc50"
      },
      "outputs": [],
      "source": [
        "multi_class = True\n",
        "load_machine_idx = 0\n",
        "DUT_idx = None\n",
        "case_index_list = [0, 1, 2, 3, 4]\n",
        "\n",
        "X_train, y_train, \\\n",
        "X_test, y_test, classes = return_datasets(data_path,\n",
        "                                          case_index_list=case_index_list,\n",
        "                                          multi_class=multi_class,\n",
        "                                          load_machine_idx=load_machine_idx,\n",
        "                                          DUT_idx=DUT_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e512e318-2f6d-47a0-b36b-eed6c1d6bc2d",
      "metadata": {
        "id": "e512e318-2f6d-47a0-b36b-eed6c1d6bc2d"
      },
      "source": [
        "## Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9c09e09-8057-4bbe-9cb2-d489a1384a22",
      "metadata": {
        "id": "c9c09e09-8057-4bbe-9cb2-d489a1384a22"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler().fit(X_train)\n",
        "\n",
        "\n",
        "X_train_scaled_ = scaler.transform(X_train)\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled_, columns=X_train.columns.values, index=X_train.index)\n",
        "\n",
        "X_test_scaled_ = scaler.transform(X_test)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled_, columns=X_test.columns.values, index=X_test.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14ecd5e9-769d-48b8-9f7e-a62515e14d0e",
      "metadata": {
        "id": "14ecd5e9-769d-48b8-9f7e-a62515e14d0e"
      },
      "source": [
        "## DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23516f77-2c5b-4df0-bf35-e481fd958fa1",
      "metadata": {
        "id": "23516f77-2c5b-4df0-bf35-e481fd958fa1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# oversampling minority classes\n",
        "oversampler = RandomOverSampler()\n",
        "\n",
        "# sliding window size\n",
        "#sw = '8s'\n",
        "sw = '8s'\n",
        "\n",
        "model_type = 'CNN'\n",
        "#batch_size = 100\n",
        "batch_size = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d1768f9-0e27-4634-b3c7-48072aea1489",
      "metadata": {
        "id": "1d1768f9-0e27-4634-b3c7-48072aea1489"
      },
      "source": [
        "### Create PyTorch Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b93e5fe7-a6c1-4252-b7ee-b12bd1243df5",
      "metadata": {
        "id": "b93e5fe7-a6c1-4252-b7ee-b12bd1243df5"
      },
      "outputs": [],
      "source": [
        "train_data = TimeSeriesData(X_train_scaled,\n",
        "                            y_train,\n",
        "                            sw=sw,\n",
        "                            class_balance=True,\n",
        "                            sampling_method=oversampler,\n",
        "                            dim_format=model_type)\n",
        "\n",
        "train_data_unbalanced = TimeSeriesData(X_train_scaled,\n",
        "                                       y_train,\n",
        "                                       sw=sw,\n",
        "                                       class_balance=False,\n",
        "                                       sampling_method=oversampler,\n",
        "                                       dim_format=model_type)\n",
        "\n",
        "test_data = TimeSeriesData(X_test_scaled,\n",
        "                           y_test,\n",
        "                           sw=sw,\n",
        "                           dim_format=model_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19486ddc-7f8f-4582-9919-e0639a9ac7b7",
      "metadata": {
        "id": "19486ddc-7f8f-4582-9919-e0639a9ac7b7"
      },
      "source": [
        "### Create DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4cb35ca-dfb9-4fe3-b4dd-32058f54f3aa",
      "metadata": {
        "id": "e4cb35ca-dfb9-4fe3-b4dd-32058f54f3aa"
      },
      "outputs": [],
      "source": [
        "pin_memory = False\n",
        "\n",
        "loader_train = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=n_cpu,\n",
        "                          pin_memory=pin_memory)\n",
        "loader_train_unbalanced = DataLoader(train_data_unbalanced, batch_size=batch_size, shuffle=False,\n",
        "                                     num_workers=n_cpu, pin_memory=pin_memory)\n",
        "loader_test = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=n_cpu,\n",
        "                         pin_memory=pin_memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a025b914-ae8c-4d1d-967f-33e14dc924ce",
      "metadata": {
        "id": "a025b914-ae8c-4d1d-967f-33e14dc924ce"
      },
      "source": [
        "### Load the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "592f7916-8c3d-4bc3-ae7c-d4ec0cc33009",
      "metadata": {
        "id": "592f7916-8c3d-4bc3-ae7c-d4ec0cc33009",
        "outputId": "a89aa7ad-cf75-4cd1-c370-e14b829cde8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ConvNet_EDL(\n",
              "  (conv1): Conv1d(131, 1000, kernel_size=(11,), stride=(1,))\n",
              "  (bn1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dropout1): Dropout(p=0.9, inplace=False)\n",
              "  (conv2): Conv1d(1000, 500, kernel_size=(11,), stride=(1,))\n",
              "  (bn2): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dropout2): Dropout(p=0.9, inplace=False)\n",
              "  (fc1): Linear(in_features=390000, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sw = '8s'\n",
        "model_type = 'CNN_EDL'\n",
        "net = torch.load(f'../models/{model_type}_CL_sw_{sw}.pt')\n",
        "net.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43c1ef59-4203-4c22-ad24-a74dbb4bc0db",
      "metadata": {
        "id": "43c1ef59-4203-4c22-ad24-a74dbb4bc0db"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b261b92-6b3e-4af9-b88b-52a86526e547",
      "metadata": {
        "id": "8b261b92-6b3e-4af9-b88b-52a86526e547"
      },
      "outputs": [],
      "source": [
        "from models import ConvNet_CL\n",
        "from models import ConvNet_EDL\n",
        "input_size = train_data[0][0].shape[0]\n",
        "seq_length = train_data[0][0].shape[1]\n",
        "print(f'Input size CNN: {input_size}')\n",
        "print(f'sequence length CNN: {seq_length}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a815d5f9-27f2-4fdd-bbf2-908ac265b7f9",
      "metadata": {
        "id": "a815d5f9-27f2-4fdd-bbf2-908ac265b7f9"
      },
      "outputs": [],
      "source": [
        "net = ConvNet_EDL(input_size, classes, seq_length, kernel_size=11, dropout=.9)\n",
        "net = net.float()\n",
        "net.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed3a0dd6-6b00-4a4f-92fb-2638ce00da11",
      "metadata": {
        "id": "ed3a0dd6-6b00-4a4f-92fb-2638ce00da11"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b1f33e7-20e5-48a3-b908-e0c57492f636",
      "metadata": {
        "id": "8b1f33e7-20e5-48a3-b908-e0c57492f636"
      },
      "outputs": [],
      "source": [
        "def KLDivergenceLoss(evidence, target):\n",
        "    alpha = evidence + 1.\n",
        "    n_classes = evidence.shape[-1]\n",
        "    alpha_tilde = target + (1 - target) * alpha\n",
        "    strength_tilde = alpha_tilde.sum(dim=-1)\n",
        "    first = (torch.lgamma(alpha_tilde.sum(dim=-1))\n",
        "             - torch.lgamma(alpha_tilde.new_tensor(float(n_classes)))\n",
        "             - (torch.lgamma(alpha_tilde)).sum(dim=-1))\n",
        "    second = (\n",
        "        (alpha_tilde - 1) *\n",
        "        (torch.digamma(alpha_tilde) - torch.digamma(strength_tilde)[:, None])\n",
        "    ).sum(dim=-1)\n",
        "    loss = (first + second)\n",
        "    return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89c5dedd-20ca-47f1-96ac-97dbe6153ac7",
      "metadata": {
        "id": "89c5dedd-20ca-47f1-96ac-97dbe6153ac7"
      },
      "outputs": [],
      "source": [
        "from labml_helpers.module import Module\n",
        "\n",
        "class TrackStatistics(Module):\n",
        "    def forward(self, evidence: torch.Tensor, target: torch.Tensor):\n",
        "        n_classes = evidence.shape[-1]\n",
        "        match = evidence.argmax(dim=-1).eq(target.argmax(dim=-1))\n",
        "        tracker.add('accuracy.', match.sum() / match.shape[0])\n",
        "        alpha = evidence + 1.\n",
        "        strength = alpha.sum(dim=-1)\n",
        "        expected_probability = alpha / strength[:, None]\n",
        "        expected_probability, _ = expected_probability.max(dim=-1)\n",
        "        uncertainty_mass = n_classes / strength\n",
        "        tracker.add('u.succ.', uncertainty_mass.masked_select(match))\n",
        "        tracker.add('u.fail.', uncertainty_mass.masked_select(~match))\n",
        "        tracker.add('prob.succ.', expected_probability.masked_select(match))\n",
        "        tracker.add('prob.fail.', expected_probability.masked_select(~match))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1a1091c-cbf5-4773-9507-7b0f829c098c",
      "metadata": {
        "id": "b1a1091c-cbf5-4773-9507-7b0f829c098c"
      },
      "outputs": [],
      "source": [
        "from labml_helpers.schedule import RelativePiecewise\n",
        "def kl_div_coef(kl_div_coef_schedule,epochs,train_dataset_size):\n",
        "    return RelativePiecewise(kl_div_coef_schedule, epochs * train_dataset_size)\n",
        "print(kl_div_coef([(0, 0.), (0.2, 0.01), (1, 1.)],1,200000))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ddfa730-76b8-42f5-933a-64ed2a64aff6",
      "metadata": {
        "id": "0ddfa730-76b8-42f5-933a-64ed2a64aff6"
      },
      "source": [
        "## Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "440300e2-e97b-48f9-ac68-c0579932f8ef",
      "metadata": {
        "id": "440300e2-e97b-48f9-ac68-c0579932f8ef"
      },
      "source": [
        "#### 1- CrossEntropyBayesRisk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4d4a956-9be7-45ac-97c5-c40850f28ed9",
      "metadata": {
        "id": "a4d4a956-9be7-45ac-97c5-c40850f28ed9"
      },
      "outputs": [],
      "source": [
        "def CrossEntropyBayesRisk(evidence, target):\n",
        "    alpha = evidence + 1.\n",
        "    strength = alpha.sum(dim=-1)\n",
        "    loss = (target * (torch.digamma(strength)[:, None] - torch.digamma(alpha))).sum(dim=-1)\n",
        "    return loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ccfee83-0178-4d75-9a47-3f2f73d12b75",
      "metadata": {
        "id": "5ccfee83-0178-4d75-9a47-3f2f73d12b75"
      },
      "source": [
        "#### 2- SquaredErrorBayesRisk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76819af7-9745-4238-884f-30a360e295b6",
      "metadata": {
        "id": "76819af7-9745-4238-884f-30a360e295b6"
      },
      "outputs": [],
      "source": [
        "def SquaredErrorBayesRisk(evidence, target):\n",
        "    alpha = evidence + 1.\n",
        "    strength = alpha.sum(dim=-1)\n",
        "    p = alpha / strength[:, None]\n",
        "    err = (target - p) ** 2\n",
        "    var = p * (1 - p) / (strength[:, None] + 1)\n",
        "    loss = (err + var).sum(dim=-1)\n",
        "    return loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71c6f0b3-7b3e-410f-82b6-befeabf00361",
      "metadata": {
        "id": "71c6f0b3-7b3e-410f-82b6-befeabf00361"
      },
      "source": [
        "#### 3- MaximumLikelihoodLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de611c41-db58-4f4c-9ba7-5300849574d6",
      "metadata": {
        "id": "de611c41-db58-4f4c-9ba7-5300849574d6"
      },
      "outputs": [],
      "source": [
        "def MaximumLikelihoodLoss(evidence, target):\n",
        "    alpha = evidence + 1.\n",
        "    strength = alpha.sum(dim=-1)\n",
        "    loss = (target * (strength.log()[:, None] - alpha.log())).sum(dim=-1)\n",
        "    return loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ae48da4-9c97-48bb-a5c4-6cd096f8905a",
      "metadata": {
        "id": "2ae48da4-9c97-48bb-a5c4-6cd096f8905a"
      },
      "source": [
        "# Model Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b18ad548-fc5a-49dc-890b-ab76c208d03a",
      "metadata": {
        "id": "b18ad548-fc5a-49dc-890b-ab76c208d03a"
      },
      "outputs": [],
      "source": [
        "lr = 1e-4\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=.4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45e5e1ad-b322-4042-9d1e-a05cf9b98b1e",
      "metadata": {
        "id": "45e5e1ad-b322-4042-9d1e-a05cf9b98b1e"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from labml import tracker\n",
        "import torch.nn as nn\n",
        "\n",
        "# intilization step\n",
        "kl_div_coef_schedule = [(0, 0.), (0.2, 0.01), (1, 1.)]\n",
        "stats = TrackStatistics()\n",
        "tracker.set_scalar(\"loss.*\", True)\n",
        "tracker.set_scalar(\"accuracy.*\", True)\n",
        "tracker.set_histogram('u.*', True)\n",
        "tracker.set_histogram('prob.*', False)\n",
        "tracker.set_scalar('annealing_coef.*', False)\n",
        "tracker.set_scalar('kl_div_loss.*', False)\n",
        "epochs = 2\n",
        "verb_it = 1000\n",
        "num_classes = len(case_index_list)\n",
        "net.train()\n",
        "\n",
        "loss_epoch = []\n",
        "acc_epoch = []\n",
        "\n",
        "losses = {\"loss\": [], \"phase\": [], \"epoch\": []}\n",
        "accuracy = {\"accuracy\": [], \"phase\": [], \"epoch\": []}\n",
        "evidences = {\"evidence\": [], \"type\": [], \"epoch\": []}\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    loss_list = []\n",
        "    acc_list = []\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_i, (X_batch, y_batch) in enumerate(loader_train):\n",
        "        tracker.add_global_step(len(loader_train.dataset))\n",
        "\n",
        "        data = X_batch.to(device)\n",
        "        y_batch = y_batch.long().view(-1).to(device)\n",
        "        # One-hot coded targets\n",
        "        eye = torch.eye(num_classes).to(torch.float).to(device)\n",
        "        target = eye[y_batch]\n",
        "        # Update global step (number of samples processed) when in training mode\n",
        "        tracker.add_global_step(len(data))\n",
        "        # Get model outputs\n",
        "        outputs = net(data)\n",
        "        # Get evidences e_t >= 0\n",
        "        outputs_to_evidence = nn.Softplus()\n",
        "        evidence = outputs_to_evidence(outputs)\n",
        "        # Calculate loss\n",
        "        loss = CrossEntropyBayesRisk(evidence,target)\n",
        "        # Calculate KL Divergence regularization loss\n",
        "        kl_div_loss = KLDivergenceLoss(evidence, target)\n",
        "        tracker.add(\"loss.\", loss)\n",
        "        tracker.add(\"kl_div_loss.\", kl_div_loss)\n",
        "        # KL Divergence loss coefficient λ_t\n",
        "        annealing_step = num_classes\n",
        "        annealing_coef = torch.min(\n",
        "            torch.tensor(1.0, dtype=torch.float32),\n",
        "            torch.tensor(epoch / num_classes, dtype=torch.float32),\n",
        "        )\n",
        "        tracker.add(\"annealing_coef.\", annealing_coef)\n",
        "        # Total loss\n",
        "        loss = loss + annealing_coef * kl_div_loss\n",
        "        # Track statistics\n",
        "        stats(evidence, target)\n",
        "        # Calculate gradients\n",
        "        loss.backward()\n",
        "        # Take optimizer step\n",
        "        optimizer.step()\n",
        "        # Clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Save the tracked metrics\n",
        "        tracker.save()\n",
        "        loss_list.append(loss.clone().cpu().detach().numpy())\n",
        "        acc = accuracy_score(y_batch.cpu(), outputs.cpu().argmax(1))\n",
        "        acc_list.append(acc)\n",
        "\n",
        "    loss_epoch.append(np.array(loss_list).mean())\n",
        "    acc_epoch.append(np.array(acc_list).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6748acdc-f403-4a72-a0e3-06a6909465df",
      "metadata": {
        "id": "6748acdc-f403-4a72-a0e3-06a6909465df"
      },
      "outputs": [],
      "source": [
        "plt.plot(loss_epoch)\n",
        "plt.plot(acc_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa322c16-35ab-41dd-aca2-542447e656f1",
      "metadata": {
        "id": "aa322c16-35ab-41dd-aca2-542447e656f1"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ca76e57-9ad9-4b76-8f96-085a1f5dabb8",
      "metadata": {
        "id": "4ca76e57-9ad9-4b76-8f96-085a1f5dabb8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from evaluation import plot_confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e08353a-c71b-4ccc-8b2b-6c03d1c15caf",
      "metadata": {
        "id": "0e08353a-c71b-4ccc-8b2b-6c03d1c15caf"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def model_evaluate(net, dataloader, device):\n",
        "    net.eval()\n",
        "    list_out = []\n",
        "    list_y = []\n",
        "    list_uncertainty = []\n",
        "    list_prob = []\n",
        "    for X_batch, y_batch in dataloader:\n",
        "\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.view(-1).to(device)\n",
        "\n",
        "        out = net(X_batch)\n",
        "\n",
        "        outputs_to_evidence = nn.Softplus()\n",
        "        evidence = outputs_to_evidence(out)\n",
        "        alpha = evidence + 1\n",
        "        uncertainty = 5 / torch.sum(alpha, dim=1, keepdim=True)\n",
        "        prob = alpha / torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "\n",
        "        list_out.append(out.cpu().detach().numpy())\n",
        "        list_y.append(y_batch.cpu().detach().numpy())\n",
        "        list_uncertainty.append(uncertainty.cpu().detach().numpy())\n",
        "        list_prob.append(prob.cpu().detach().numpy())\n",
        "\n",
        "    y_score = np.vstack(list_out)\n",
        "    y_labels = np.hstack(list_y)\n",
        "    y_uncertainty = np.vstack(list_uncertainty)\n",
        "    y_prob = np.vstack(list_prob)\n",
        "\n",
        "    return y_score, y_labels, y_uncertainty, y_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15f8a083-5c20-47e0-aafb-b234a26f9b14",
      "metadata": {
        "id": "15f8a083-5c20-47e0-aafb-b234a26f9b14"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def model_evaluate(net, dataloader, device):\n",
        "    net.eval()\n",
        "    list_out = []\n",
        "    list_y = []\n",
        "    list_uncertainty = []\n",
        "    list_prob = []\n",
        "    list_predict = []\n",
        "    for batch_i,(X_batch, y_batch) in enumerate(dataloader):\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.view(-1).to(device)\n",
        "        out = net(X_batch)\n",
        "\n",
        "        outputs_to_evidence = nn.ReLU()\n",
        "        evidence = outputs_to_evidence(out)\n",
        "        alpha = evidence + 1\n",
        "        uncertainty = 5 / torch.sum(alpha, dim=1, keepdim=True)\n",
        "        _, preds = torch.max(out, 1)\n",
        "        prob = alpha / torch.sum(alpha, dim=1, keepdim=True)\n",
        "        out = out.flatten()\n",
        "        prob = prob.flatten()\n",
        "        preds = preds.flatten()\n",
        "\n",
        "        list_out.append(out.cpu().detach().numpy())\n",
        "        list_y.append(y_batch.cpu().detach().numpy())\n",
        "        list_uncertainty.append(uncertainty.cpu().detach().numpy())\n",
        "        list_prob.append(prob.cpu().detach().numpy())\n",
        "\n",
        "    y_score = np.vstack(list_out)\n",
        "    y_labels = np.hstack(list_y)\n",
        "    y_uncertainty = np.vstack(list_uncertainty)\n",
        "    y_prob = np.vstack(list_prob)\n",
        "\n",
        "    return y_score, y_labels,y_uncertainty,y_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a0b2ce5-b039-4a4c-88c9-bb71e73b3452",
      "metadata": {
        "id": "3a0b2ce5-b039-4a4c-88c9-bb71e73b3452",
        "outputId": "f0476237-f6a1-48d6-dad0-884d3c54c81b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
          ]
        }
      ],
      "source": [
        "y_score, y_labels,u_list,y_prob = model_evaluate(net, loader_test, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03d83b2e-7aa9-4a82-8b95-4ab594babce7",
      "metadata": {
        "id": "03d83b2e-7aa9-4a82-8b95-4ab594babce7"
      },
      "outputs": [],
      "source": [
        "print(np.shape(y_prob))\n",
        "print(np.shape(y_score))\n",
        "print(np.shape(y_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db076671-17f2-4a90-9dcb-1dd664fb24a1",
      "metadata": {
        "id": "db076671-17f2-4a90-9dcb-1dd664fb24a1"
      },
      "outputs": [],
      "source": [
        "y_predicted_labels= y_score.argmax(1).reshape(-1,1)\n",
        "acc = accuracy_score(y_labels, y_predicted_labels)\n",
        "print('Accuracy', acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d4e87e7-3017-4389-b719-98c5e8ce3ab2",
      "metadata": {
        "id": "4d4e87e7-3017-4389-b719-98c5e8ce3ab2"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_labels, y_predicted_labels, digits=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4cc1ed8-2e6e-4819-a8e4-9733efb04fbb",
      "metadata": {
        "id": "e4cc1ed8-2e6e-4819-a8e4-9733efb04fbb"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4654943b-e991-4c98-b66a-50f7cee6a5ba",
      "metadata": {
        "id": "4654943b-e991-4c98-b66a-50f7cee6a5ba"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_labels, y_predicted_labels, labels=classes, normalize='true')\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.grid(False)\n",
        "plot_confusion_matrix(cm, classes=classes, title='CM')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cf62345-f665-46c6-ae40-e4b0b9baf2ec",
      "metadata": {
        "id": "9cf62345-f665-46c6-ae40-e4b0b9baf2ec"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe4223b2-6ad6-4b65-a75f-b9e6c71aadc8",
      "metadata": {
        "id": "fe4223b2-6ad6-4b65-a75f-b9e6c71aadc8"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('../models/'):\n",
        "    os.mkdir('../models/')\n",
        "model_type = \"CNN_EDL_Full_batchId\"\n",
        "# Save model weights in PyTorch format\n",
        "torch.save(net, f'../models/{model_type}_CL_sw_{sw}.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd6907fe-7da6-4f3f-a817-cfc307c92120",
      "metadata": {
        "id": "dd6907fe-7da6-4f3f-a817-cfc307c92120"
      },
      "source": [
        "# Plot predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8fba07c-b02b-449d-8097-0e2903e372fb",
      "metadata": {
        "id": "d8fba07c-b02b-449d-8097-0e2903e372fb"
      },
      "outputs": [],
      "source": [
        "pred_df = pd.DataFrame(y_score, columns=[0,1,2,3,4], index=test_data.labels.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df0894b8-2c49-4615-82e8-e094f401e3f7",
      "metadata": {
        "tags": [],
        "id": "df0894b8-2c49-4615-82e8-e094f401e3f7"
      },
      "outputs": [],
      "source": [
        "for name, group in pred_df.groupby(level=y_test.index.names[:-1]):\n",
        "    fig, ax = plt.subplots(figsize=(10,5))\n",
        "    pred_df.loc[name, :].plot(ax=ax)\n",
        "    y_test.loc[name, :].plot(ax=ax)\n",
        "    u_list_df.loc[name, :].plot(ax=ax)\n",
        "    plt.ylabel('class prob')\n",
        "    plt.title(f'Time Series ID: {name}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2f7652f-05d7-4ce5-a8d5-5331b147da79",
      "metadata": {
        "id": "c2f7652f-05d7-4ce5-a8d5-5331b147da79"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}