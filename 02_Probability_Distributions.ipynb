{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zain-mahfoud94/Python-Uni/blob/main/02_Probability_Distributions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc9lg-szKkvC"
      },
      "source": [
        "# Probability Distributions\n",
        "\n",
        "**Density Estimation**:\n",
        "Modeling the probability distribution $p(\\mathbf{x})$ of a random variable $\\mathbf{x}$, given a finite set $\\mathbf{x}_1, \\dots, \\mathbf{x}_N$ of observations.\n",
        "For this task of density estimation, we assume that these data points are independently and identically distributed (i. i. d.).\n",
        "\n",
        "### Table of Contents\n",
        "1. [Setup](#setup)\n",
        "2. [Binary Variables](#binary)\n",
        "3. [Mulitnomial Variables (Optional)](#multinomial)\n",
        "4. [Gaussian Distribution](#gaussian)\n",
        "5. [Nonparametric Methods](#kde)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCvhe4NEKkvJ"
      },
      "source": [
        "## Setup <a class=\"anchor\" id=\"setup\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziigLbxwKkvL"
      },
      "outputs": [],
      "source": [
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKBXBXsQKkvO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pylab as plt\n",
        "from ipywidgets import interact\n",
        "\n",
        "def get_data(n_samples=20):\n",
        "    np.random.seed(0)\n",
        "    return (np.random.rand(n_samples) > 0.5).astype(int)\n",
        "\n",
        "def get_data_kde():\n",
        "    X = np.r_[np.random.normal(-3, 1, 50), np.random.normal(3, 1, 50)]\n",
        "    return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTPpu_kZKkvP"
      },
      "source": [
        "## Binary Variables <a class=\"anchor\" id=\"binary\"></a>\n",
        "### Bernoulli Distribution\n",
        "- Implement a class representing the `BernoulliDistribution`. It should contain the following methods:\n",
        "  - `__init__(self, mu)` - Initialization method that verifies that $0 \\leq \\mu \\leq 1$.\n",
        "  - `prob(self, x)` - Should return the probability of a binary sample $x \\in \\{0, 1\\}$: $$\\operatorname{Bern}(x \\mid \\mu)=\\mu^{x}(1-\\mu)^{1-x}.$$\n",
        "  - `log_prob(self, x)` - Should return the log-probability of a binary sample: $$\\ln \\operatorname{Bern}(x \\mid \\mu)= \\ln \\left(\\mu^{x}(1-\\mu)^{1-x} \\right)$$\n",
        "  This equation should be simplified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd00jM5gKkvP"
      },
      "outputs": [],
      "source": [
        "# Bernoulli distribution\n",
        "class BernoulliDistribution:\n",
        "    def __init__(self, mu):\n",
        "        assert 0 <= mu <= 1, 'The parameter has to be between 0 and 1.'\n",
        "        self.mu = mu\n",
        "\n",
        "    def prob(self, x):\n",
        "        assert x in {0, 1}, 'The input has to be binary.'\n",
        "        return self.mu**x * (1-self.mu)**(1-x)\n",
        "\n",
        "    def log_prob(self, x):\n",
        "        assert x in {0, 1}, 'The input has to be binary.'\n",
        "        # Due to the log, we have to ensure that mu wont be 0 or 1.\n",
        "        # In such cases, we will adjust its value by a tiny offset using `np.clip`.\n",
        "        mu = np.clip(self.mu, 1e-5, 1-1e-5)\n",
        "        return x * np.log(mu) + (1 - x)* np.log(1 - mu)\n",
        "\n",
        "bern = BernoulliDistribution(mu=0.2)\n",
        "bern.prob(0), bern.log_prob(0)  # Should return (0.8, -0.2231435513142097)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwe7mVOMKkvQ"
      },
      "source": [
        "Consider the following observations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHwXXam3KkvR"
      },
      "outputs": [],
      "source": [
        "X = get_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-_r-xs7KkvS"
      },
      "source": [
        "On the assumption that the observations are drawn independently, we can construct the likelihood function\n",
        "$$p(\\mathcal{D} \\mid \\mu)=\\prod_{n=1}^{N} p\\left(x_{n} \\mid \\mu\\right)=\\prod_{n=1}^{N} \\mu^{x_{n}}(1-\\mu)^{1-x_{n}}.$$\n",
        "- Implement a function `likelihood` defining the equation above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQbXTSs2KkvT"
      },
      "outputs": [],
      "source": [
        "# Specify a Likelihood based on the data\n",
        "def likelihood(mu, X):\n",
        "    bern = BernoulliDistribution(mu)\n",
        "    return np.prod([bern.prob(x) for x in X])\n",
        "likelihood(.5, [1, 1, 1])  # Should return 0.125"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ6rmc3dKkvT"
      },
      "source": [
        "The resulting likelihood may show a tiny values, which may lead to numerical problems during the optimization (e.g., when we have big number of samples). To ensure numerical stability as well as ease further calculation, we will make use of the logarithm as it does not change the optimum of a function.\n",
        "- Implement a function `log_likelihood` returning the log likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4Z8AuzUKkvT"
      },
      "outputs": [],
      "source": [
        "# Specify a Likelihood based on the data\n",
        "def log_likelihood(mu, X):\n",
        "    bern = BernoulliDistribution(mu)\n",
        "    return np.sum([bern.log_prob(x) for x in X])\n",
        "log_likelihood(0.5, [1, 1, 1])  # Should return -2.0794415416798357"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJVgYEttKkvU"
      },
      "source": [
        "- Plot the two different likelihood functions for different values of $0 \\leq \\mu \\leq 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLviPtnmKkvU"
      },
      "outputs": [],
      "source": [
        "axis = np.linspace(0, 1, 201)\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title('Likelihood')\n",
        "axis_likelihood = [likelihood(mu, X) for mu in axis]\n",
        "plt.plot(axis, axis_likelihood)\n",
        "\n",
        "# Verify that their max value are the same\n",
        "idx_max = np.argmax(axis_likelihood)\n",
        "maximum = axis[idx_max]\n",
        "plt.vlines(maximum, *plt.ylim(), label='Max = {:.2f}'.format(maximum))\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Log Likelihood')\n",
        "axis_log_likelihood = [log_likelihood(mu, X) for mu in axis]\n",
        "plt.plot(axis, axis_log_likelihood)\n",
        "\n",
        "# Verify that their max value are the same\n",
        "idx_max = np.argmax(axis_log_likelihood)\n",
        "maximum = axis[idx_max]\n",
        "plt.vlines(maximum, *plt.ylim(), label='Max = {:.2f}'.format(maximum))\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MeeDsXlKkvU"
      },
      "source": [
        "If we set the derivative of $\\ln p(\\mathcal{D} | \\mu)$ with respect to $\\mu$ equal to zero, we obtain the maximum likelihood estimator $$\\mu_{\\mathrm{ML}}=\\frac{1}{N} \\sum_{n=1}^{N} x_{n}.$$\n",
        "- Compute the maximum likelihood estimator using our observations and verify visually that it corresponds to the maximum in both plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Z9T1H_AKkvV"
      },
      "outputs": [],
      "source": [
        "# ML Solution\n",
        "mu_ml = np.sum(X) / len(X)\n",
        "mu_ml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glAIRzTIKkvV"
      },
      "source": [
        "### Binomial Distribution\n",
        "- Implement the class `BinomialDistribution` containing the following methods:\n",
        "  - `__init__(self, n, mu)` - Initialization method saving parameters and verifying their domain.\n",
        "  - `prob(self, x)` - Should return the probability:  $$\\operatorname{Bin}(m \\mid N, \\mu)=\\left(\\begin{array}{l} N \\\\ m \\end{array}\\right) \\mu^{m}(1-\\mu)^{N-m}.$$\n",
        "  The normalization coefficient $$ \\left(\\begin{array}{l} N \\\\ m \\end{array}\\right) \\equiv \\frac{N !}{(N-m) ! m !}$$ can be implemented using `np.math.factorial`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2_1u3SUKkvV"
      },
      "outputs": [],
      "source": [
        "class BinomialDistribution:\n",
        "    def __init__(self, n, mu):\n",
        "        assert 0 <= mu <= 1, 'The parameter has to be between 0 and 1.'\n",
        "        self.n = n\n",
        "        self.mu = mu\n",
        "        self.factorial = np.math.factorial\n",
        "\n",
        "    def prob(self, x):\n",
        "        assert 0 <= x <= 10, 'The input should be between 0 and {}.'.format(self.n)\n",
        "        norm_coefficient = self.factorial(self.n) / (self.factorial(self.n - x)*self.factorial(x))\n",
        "        unnorm_prob = self.mu**x * (1-self.mu)**(self.n - x)\n",
        "        return norm_coefficient * unnorm_prob\n",
        "\n",
        "binom = BinomialDistribution(10 , .5)\n",
        "binom.prob(5)  # Should return 0.24609375"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMDbIJVvKkvW"
      },
      "source": [
        "- Plot the binomial distribution ($N=10, \\mu=.25$) by using a bar plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0bR2l2eKkvW"
      },
      "outputs": [],
      "source": [
        "binom = BinomialDistribution(10 , .25)\n",
        "inps = np.arange(0, 11)\n",
        "probs = [binom.prob(x) for x in inps]\n",
        "plt.bar(inps, probs)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('probability')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkB41WCMKkvW"
      },
      "source": [
        "### Beta Distribution\n",
        "- Implement the class `BetaDistribution` containing the following methods:\n",
        "  - `__init__(self, a, b)` - Initialization method saving parameters and verifying their domain.\n",
        "  - `prob(self, x)` - Should return the probability: $$\\operatorname{Beta}(\\mu \\mid a, b)=\\frac{\\Gamma(a+b)}{\\Gamma(a) \\Gamma(b)} \\mu^{a-1}(1-\\mu)^{b-1}$$\n",
        "  The normalization coefficient $$ \\frac{\\Gamma(a+b)}{\\Gamma(a) \\Gamma(b)}$$ can be implemented using `np.math.gamma`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaLBV2B2KkvW"
      },
      "outputs": [],
      "source": [
        "class BetaDistribution:\n",
        "    def __init__(self, a, b):\n",
        "        assert a > 0 and b > 0, 'Both paramters have to be greater than 0.'\n",
        "        self.a = a\n",
        "        self.b = b\n",
        "        self.gamma = np.math.gamma\n",
        "\n",
        "    def prob(self, x):\n",
        "        assert 0 < x < 1, 'Input has to be between 0 and 1.'\n",
        "        norm_coefficent = self.gamma(self.a + self.b) / (self.gamma(self.a)*self.gamma(self.b))\n",
        "        unnorm_prob = x**(self.a - 1) * (1 - x)**(self.b-1)\n",
        "        return norm_coefficent*unnorm_prob\n",
        "\n",
        "\n",
        "beta = BetaDistribution(1, 3)\n",
        "beta.prob(.5)  # Should return 0.75"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox-Eo1XYKkvX"
      },
      "source": [
        "- Plot the distribution for the given parameter settings:\n",
        "  - $a=0.1\\quad b=0.1$\n",
        "  - $a=1\\quad b=1$\n",
        "  - $a=2\\quad b=3$\n",
        "  - $a=8\\quad b=4$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWV7VotSKkvX"
      },
      "outputs": [],
      "source": [
        "axis = np.linspace(1e-2, 1-1e-2, 201)\n",
        "\n",
        "plt.subplot(221)\n",
        "plt.title('a = {}, b = {}'.format(0.1, 0.1))\n",
        "beta = BetaDistribution(.1, .1)\n",
        "plt.plot(axis, [beta.prob(x) for x in axis])\n",
        "\n",
        "plt.subplot(222)\n",
        "plt.title('a = {}, b = {}'.format(1, 1))\n",
        "beta = BetaDistribution(1, 1)\n",
        "plt.plot(axis, [beta.prob(x) for x in axis])\n",
        "\n",
        "plt.subplot(223)\n",
        "plt.title('a = {}, b = {}'.format(2, 3))\n",
        "beta = BetaDistribution(2, 3)\n",
        "plt.plot(axis, [beta.prob(x) for x in axis])\n",
        "\n",
        "plt.subplot(224)\n",
        "plt.title('a = {}, b = {}'.format(8, 4))\n",
        "beta = BetaDistribution(8, 4)\n",
        "plt.plot(axis, [beta.prob(x) for x in axis])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpEmyXqmKkvX"
      },
      "source": [
        "### Bayesian Approach for Parameter Estimation\n",
        "In a Bayesian approach, the posterior distribution of $\\mu$ is now determined by multiplying the prior distribution $\\operatorname{Beta}(\\mu \\mid a, b)$ with the likelihood function $\\operatorname{Bin}(m \\mid N, \\mu)$ and subsequent normalization.\n",
        "\n",
        "The observations below encode a coin toss ($0$ = tails, $1$ = heads):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdL-AEylKkvX"
      },
      "outputs": [],
      "source": [
        "X = [1, 1, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEXZ0bDyKkvY"
      },
      "source": [
        "- Define a meaningful prior distribution\n",
        "- Define the likelihood function\n",
        "- Define the posterior distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk6UGOkSKkvY"
      },
      "outputs": [],
      "source": [
        "prior_dist = BetaDistribution(2, 2)\n",
        "\n",
        "def likelihood_func(mu, X) :\n",
        "    return np.prod([BernoulliDistribution(mu).prob(x) for x in X])\n",
        "\n",
        "posterior_distribution = BetaDistribution(\n",
        "    a=prior_dist.a + sum(X),\n",
        "    b=prior_dist.b + len(X)-sum(X),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGzGc1GiKkvY"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "axis = np.linspace(1e-2, 1-1e-2, 201)\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.title('Prior Distribution')\n",
        "plt.plot(axis, [prior_dist.prob(mu) for mu in axis])\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.title('Likelihood Function')\n",
        "plt.plot(axis, [likelihood_func(mu, X) for mu in axis])\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.title('Posterior Distribution')\n",
        "plt.plot(axis, [posterior_distribution.prob(mu) for mu in axis])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXyaTd5mKkvY"
      },
      "source": [
        "If our goal is to predict, as best we can, the outcome of the next trial, then we must evaluate the predictive distribution of $x$, given the observed data set $\\mathcal{D}$. From the sum and product rules of probability, follows\n",
        "$$p(x=1 \\mid \\mathcal{D})=\\int_{0}^{1} p(x=1 \\mid \\mu) p(\\mu \\mid \\mathcal{D}) \\mathrm{d} \\mu=\\int_{0}^{1} \\mu p(\\mu \\mid \\mathcal{D}) \\mathrm{d} \\mu=\\mathbb{E}[\\mu \\mid \\mathcal{D}]$$\n",
        "\n",
        "That is, the optimal prediction for $1$ is given by the mean of the beta distribution:\n",
        "$$p(x=1 \\mid \\mathcal{D})=\\frac{m+a}{m+a+l+b}$$\n",
        "- Implement a `predict` function that uses the parameters of the posterior distribution to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Be4NQSZKkvZ"
      },
      "outputs": [],
      "source": [
        "def predict(x, posterior_distribution):\n",
        "    assert x in {0, 1}, 'Input x is not binary.'\n",
        "    a, b = posterior_distribution.a, posterior_distribution.b\n",
        "    prob = a / (a+b)\n",
        "    if x != 1:\n",
        "        prob = 1 - prob\n",
        "    return prob\n",
        "predict(1, posterior_distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxLdPFbcKkvZ"
      },
      "outputs": [],
      "source": [
        "# Example that the posterior distribution will be close to the likelihood\n",
        "# if the amount of samples is big enough.\n",
        "from ipywidgets import interact\n",
        "def get_data(n_samples):\n",
        "    np.random.seed(100)\n",
        "    return (np.random.rand(n_samples) > .8).astype(int)\n",
        "\n",
        "@interact(n_samples=(1, 100, 1))\n",
        "def tmp(n_samples=1):\n",
        "    X = get_data(n_samples)\n",
        "    prior_dist = BetaDistribution(5, 2)\n",
        "    def likelihood_func(mu, X) :\n",
        "        return np.prod([BernoulliDistribution(mu).prob(x) for x in X])\n",
        "    posterior_distribution = BetaDistribution(\n",
        "        a=prior_dist.a + sum(X),\n",
        "        b=prior_dist.b + len(X)-sum(X),\n",
        "    )\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    axis = np.linspace(1e-2, 1-1e-2, 201)\n",
        "\n",
        "    plt.subplot(131)\n",
        "    plt.title('Prior Distribution')\n",
        "    plt.plot(axis, [prior_dist.prob(mu) for mu in axis])\n",
        "\n",
        "    plt.subplot(132)\n",
        "    plt.title('Likelihood Function')\n",
        "    plt.plot(axis, [likelihood_func(mu, X) for mu in axis])\n",
        "\n",
        "    plt.subplot(133)\n",
        "    plt.title('Posterior Distribution')\n",
        "    plt.plot(axis, [posterior_distribution.prob(mu) for mu in axis])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfU_zBamKkvZ"
      },
      "source": [
        "## Multinomial Variables (Optional) <a class=\"anchor\" id=\"multinomial\"></a>\n",
        "* Implement the following distributions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jix5R4teKkvZ"
      },
      "source": [
        "### Categorical Distribution\n",
        "$$p(\\mathbf{x} \\mid \\boldsymbol{\\mu})=\\prod_{k=1}^{K} \\mu_{k}^{x_{k}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ica2mx2LKkva"
      },
      "outputs": [],
      "source": [
        "# Generalization Bernoulli\n",
        "class Categorical:\n",
        "    def __init__(self, mu):\n",
        "        self.mu = np.array(mu)\n",
        "        assert np.all(self.mu >= 0), 'All elements have to be greater or equal 0.'\n",
        "        assert np.sum(self.mu) == 1, 'The parameter has to sum to 1.'\n",
        "\n",
        "    def prob(self, x):\n",
        "        assert sum(x) == 1 and 1 in x, 'Input has to be 1-of-K encoded.'\n",
        "        assert len(x) == len(self.mu), 'Input and parameter vector have to be the same length.'\n",
        "        return np.prod([mu_k**x_k for x_k, mu_k in zip(x, self.mu)])\n",
        "\n",
        "cat_dist = Categorical(mu=[.5, .5, 0])\n",
        "cat_dist.prob([0, 1, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdokUHj2Kkva"
      },
      "source": [
        "### Multinomial Distribution\n",
        "$$\n",
        "\\operatorname{Mult}\\left(m_{1}, m_{2}, \\ldots, m_{K} \\mid \\boldsymbol{\\mu}, N\\right)=\\left(\\begin{array}{c}\n",
        "N \\\\\n",
        "m_{1}, m_{2}, \\ldots, m_{k}\n",
        "\\end{array}\\right) \\prod_{k=1}^{K} \\mu_{k}^{m_{k}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrA5OgM6Kkva"
      },
      "outputs": [],
      "source": [
        "class Multinomial:\n",
        "    def __init__(self, mu, n):\n",
        "        self.mu = np.array(mu)\n",
        "        self.n = np.array(n)\n",
        "        self.factorial = np.math.factorial\n",
        "        assert np.all(self.mu >= 0), 'All elements of mu have to be greater or equal 0.'\n",
        "        assert np.sum(self.mu) == 1, 'The parameter mu has to sum to 1.'\n",
        "        assert self.n > 0, 'Number of trials has to be greater than 0.'\n",
        "\n",
        "    def prob(self, x):\n",
        "        assert sum(x) == self.n\n",
        "        norm_coef = self.factorial(self.n) / np.prod([self.factorial(x_k) for x_k in x])\n",
        "        unnorm_prob = np.prod([mu_k**x_k for x_k, mu_k in zip(x, self.mu)])\n",
        "        return norm_coef*unnorm_prob\n",
        "\n",
        "mult = Multinomial([.2, .3, .5], 6)\n",
        "mult.prob([1, 2, 3])  #  should be 0.13499999999999998"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI7VCc7TKkvb"
      },
      "source": [
        "### Dirichlet Distribution\n",
        "$$\\operatorname{Dir}(\\boldsymbol{\\mu} \\mid \\boldsymbol{\\alpha})=\\frac{\\Gamma\\left(\\alpha_{0}\\right)}{\\Gamma\\left(\\alpha_{1}\\right) \\cdots \\Gamma\\left(\\alpha_{K}\\right)} \\prod_{k=1}^{K} \\mu_{k}^{\\alpha_{k}-1}$$\n",
        "where $\\alpha_{0}=\\sum_{k=1}^{K} \\alpha_{k}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9R3KppuBKkvb"
      },
      "outputs": [],
      "source": [
        "class Dirichlet:\n",
        "    def __init__(self, alpha):\n",
        "        self.alpha = np.array(alpha)\n",
        "        assert all(self.alpha > 0), 'All elements of alpha have to be positive.'\n",
        "\n",
        "        # Compute the coefficient for normalization\n",
        "        gamma = np.math.gamma\n",
        "        alpha0 = sum(self.alpha)\n",
        "        self.norm_coef = gamma(alpha0) / np.prod([gamma(a) for a in alpha])\n",
        "\n",
        "    def prob(self, x):\n",
        "        x = np.array(x)\n",
        "        assert all(0 < x) and all(x < 1) and sum(x) == 1, 'Input has to be a probability vector.'\n",
        "        unnorm_prob = np.prod([mu_k**(a_k-1) for mu_k, a_k in zip(x, self.alpha)])\n",
        "        return self.norm_coef * unnorm_prob\n",
        "\n",
        "dirichlet = Dirichlet([.5, .5, .5])\n",
        "dirichlet.prob([.1, .1, .8]) # should be 1.7794063585429432"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q4IrtpnKkvj"
      },
      "source": [
        "## Gaussian Distribution <a class=\"anchor\" id=\"gaussian\"></a>\n",
        "* Implement the following distributions:\n",
        "$$\\mathcal{N}\\left(x \\mid \\mu, \\sigma^{2}\\right)=\\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{1 / 2}} \\exp \\left\\{-\\frac{1}{2 \\sigma^{2}}(x-\\mu)^{2}\\right\\}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkzn1k5IKkvj"
      },
      "outputs": [],
      "source": [
        "class Gaussian:\n",
        "    def __init__(self, mu, sigma):\n",
        "        self.mu = np.array(mu)\n",
        "        self.sigma = np.array(sigma)\n",
        "        assert self.sigma > 0, 'The standard deviation has to be greater than 0.'\n",
        "\n",
        "        # Computation of the normalization coefficient\n",
        "        self.norm_coef = 1 / np.sqrt(2*np.pi*self.sigma**2)\n",
        "\n",
        "    def prob(self, x):\n",
        "        unnorm_prob = np.exp(- (x - self.mu)**2 / (2*self.sigma**2))\n",
        "        return self.norm_coef * unnorm_prob\n",
        "\n",
        "gaussian = Gaussian(0, 1)\n",
        "gaussian.prob(0)  # should be 0.3989422804014327"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyQgxHooKkvj"
      },
      "source": [
        "$$\\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}, \\mathbf{\\Sigma})=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right\\}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-Up0GU3Kkvj"
      },
      "outputs": [],
      "source": [
        "class MultivariateGaussian:\n",
        "    def __init__(self, mu, cov):\n",
        "        self.mu = np.array(mu)\n",
        "        self.cov = np.array(cov)\n",
        "\n",
        "        # For the Gaussian distribution to be normalized, all eigenvalues of the\n",
        "        # covariance matrix must be strictly positive.\n",
        "        # A matrix with strictly positive eigenvalues is called positive definite.\n",
        "        eig_vals = np.linalg.eigvals(self.cov)\n",
        "        assert np.all(eig_vals > 0), 'Matrix is not positive definite.'\n",
        "\n",
        "        self.cov_inv = np.linalg.inv(cov)\n",
        "        self.D = len(self.mu)\n",
        "        self.norm_coef = (1 / (2*np.pi)**(self.D/2)) * (1 / np.prod(np.sqrt(eig_vals)))\n",
        "\n",
        "    def prob(self, x):\n",
        "        v = np.subtract(x, self.mu)\n",
        "        unnorm_prob = np.exp(- (v @ self.cov_inv @ v) / 2)\n",
        "        return self.norm_coef * unnorm_prob\n",
        "\n",
        "multivariate_gaussian= MultivariateGaussian([0, 0], [[1, 0], [0, 1]])\n",
        "multivariate_gaussian.prob([0, 0])  # should be 0.15915494309189535"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shrDzHyhKkvk"
      },
      "source": [
        "## Nonparametric Methods <a class=\"anchor\" id=\"kde\"></a>\n",
        "Observations are being drawn from some unknown probability density $p(\\mathbf{x})$ in some $D$-dimensional space, which we shall assume to be Euclidean.\n",
        "Goal: Estimate the value of $p(\\mathbf{x})$.\n",
        "\n",
        "### Kernel Density Estimators\n",
        "As shown in the lecture, we can estimate the density by $$p(\\mathbf{x})=\\frac{K}{N V}$$ where\n",
        "- $N$ is the number of observations,\n",
        "- $K$ is the number of observations that lie inside some region $\\mathcal{R}$, and\n",
        "- $V$ is the volume of region $\\mathcal{R}$.\n",
        "\n",
        "By fixing $V$ and determining $K$ from the data, we obtain the *kernel density estimator*.\n",
        "\n",
        "For the kernel density method, we assume that the region $\\mathcal{R}$ is a small hypercube that has its center at the point $\\mathbf{x}$ for which the density is to be determined.\n",
        "\n",
        "In order to count the number $K$ of points falling within this region, it is convenient to define the following function:\n",
        "\n",
        "$$k(\\mathbf{u})=\\left\\{\\begin{array}{ll}\n",
        "1, & \\left|u_{i}\\right| \\leq 1 / 2, \\quad i=1, \\ldots, D \\\\\n",
        "0, & \\text { otherwise }\n",
        "\\end{array}\\right.$$\n",
        "which represents a unit cube centered on the origin.\n",
        "\n",
        "For the given data set, the total number of data points lying inside this cube will therefore be\n",
        "\n",
        "$$K=\\sum_{n=1}^{N} k\\left(\\frac{\\mathbf{x}-\\mathbf{x}_{n}}{h}\\right)$$\n",
        "\n",
        "When substituted into the estimation formula $p(x) =K/(NV)$, the following result is obtained for estimating the density at point $\\mathbf{x}$:\n",
        "\n",
        "$$p(\\mathbf{x})=\\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{h^{D}} k\\left(\\frac{\\mathbf{x}-\\mathbf{x}_{n}}{h}\\right)$$\n",
        "where we have used $V=h^D$ for the volume of a hypercube of side $h$ in $D$ dimensions.\n",
        "\n",
        "- Implement the kernel density estimator we described above for the following observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STxjJJBnKkvk"
      },
      "outputs": [],
      "source": [
        "X = get_data_kde()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAWmBxzxKkvk"
      },
      "outputs": [],
      "source": [
        "class KernelDensityEstimator:\n",
        "    def __init__(self, data, h):\n",
        "        self.X = np.array(data)\n",
        "        self.h = np.array(h)\n",
        "        self.D = 1  # Here only for the 1d data set\n",
        "\n",
        "        assert self.h > 0, 'The length of the cube needs to be greater than 0.'\n",
        "\n",
        "    def prob(self, x):\n",
        "        x = np.array(x)\n",
        "        N = len(self.X)\n",
        "        V = self.h**self.D\n",
        "        # Count the number of samples falling inside the unit cube\n",
        "        K = np.sum(self.k((x - self.X) / self.h))\n",
        "\n",
        "        p = K / (V * N)\n",
        "        return p\n",
        "\n",
        "    def k(self, u):\n",
        "        norm = np.abs(u)\n",
        "        return (norm <= .5).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z9AMNwgKkvk"
      },
      "source": [
        "- Plot the data using a histogram and plot the density using your implemented kernel density estimator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPkJy4paKkvl"
      },
      "outputs": [],
      "source": [
        "@interact(h=(0.1, 10, 0.1))\n",
        "def plot(h=1):\n",
        "    kde = KernelDensityEstimator(data=X, h=h)\n",
        "    axis = np.linspace(-8, 8, 201)\n",
        "    plt.plot(axis, [kde.prob(x) for x in axis], linewidth=3, zorder=4, color='blue')\n",
        "    plt.hist(X, zorder=3, density=True)\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "toc-autonumbering": true,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}