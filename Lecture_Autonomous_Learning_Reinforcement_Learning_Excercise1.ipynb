{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zain-mahfoud94/Python-Uni/blob/main/Lecture_Autonomous_Learning_Reinforcement_Learning_Excercise1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Excercise: Reinforcement Learning\n",
        "\n",
        "by Christoph Scholz (christoph.scholz@iee.fraunhofer.de)\n",
        "\n",
        "### Lecture: Autonomous Learning, WS 2021/22\n"
      ],
      "metadata": {
        "id": "uKLEJng9jQRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1\n",
        "Can the optimal policy change if you modify the discount factor? Justify your answer.\n",
        "\n",
        "Answer: The discount factor is a kind of measure of how much future rewards are weighted in relation to the present. If the discount factor is close to one, then future rewards are valued equally than current rewards. If the discount factor is close to 0, then only immediate rewards are taken into account. The discount factor significantly influences the determination of the optimal strategy. If the discount factor is small, the agent only learns actions that lead to short-term rewards; if the discount factor is high, the agent can also learn actions that lead to high rewards in the long term, even if these may not seem optimal in the short term.  "
      ],
      "metadata": {
        "id": "aueKVS4aj3PO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2:\n",
        "How is Reinforcement Learning different from supervised and unsupervised learning?\n",
        "\n",
        "Answer:\n",
        "- In Reinforcement Learning, the goal is to learn an optimal strategy, while in supervised learning, the goal is to learn a function that maps an input to an output (based on training examples). In unsupervised learning the goal is to detect hidden patterns in the data.\n",
        "- In RL the agent needs to find a good balance between exploration and exploitation. In supervised and unsupervised learning learning in general we do not need to care about exploration.\n",
        "- In RL the agent does not know the right answer, in supervised learning we have labled data."
      ],
      "metadata": {
        "id": "R9_LUq-1z2UA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3:\n",
        "To process this task, please work through the following steps\n",
        "1. Install and Import the OpenAI Gym's CartPole-v0 Environment\n",
        "2. Implement a random agent and determine the average number of steps the agent survives.\n",
        "3. Define your own policies and analyze the number of steps your agent survives.\n",
        "4. Test the same using the MountainCar-v0 - Enviroment"
      ],
      "metadata": {
        "id": "MiFzUxm_ktxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and import required Packages"
      ],
      "metadata": {
        "id": "6EpkU3IuoL_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "\n",
        "import gym\n",
        "import random\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "28hdgqp8oRjd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "988255e6-9a5e-459e-f2a0-00fb27080bb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "obs = env.reset()"
      ],
      "metadata": {
        "id": "C82nELzVpz1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.seed(42)\n",
        "\n",
        "def basic_policy(obs):\n",
        "    angle = obs[2]\n",
        "    return 0 if angle < 0 else 1\n",
        "\n",
        "def random_policy(obs):\n",
        "  return random.randint(0,1)\n",
        "\n",
        "totals = []\n",
        "for episode in range(500):\n",
        "    episode_rewards = 0\n",
        "    obs = env.reset()\n",
        "    for step in range(200):\n",
        "        action = basic_policy(obs)\n",
        "        #action = random_policy(obs)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        episode_rewards += reward\n",
        "        if done:\n",
        "            break\n",
        "    totals.append(episode_rewards)\n"
      ],
      "metadata": {
        "id": "Wl2IqIcRmWI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Policy"
      ],
      "metadata": {
        "id": "t4nFpCNWqwbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(totals), np.mean(totals), np.median(totals), np.min(totals)"
      ],
      "metadata": {
        "id": "08uBJCDgsXaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d66fecf1-4d22-4644-83e0-b5799e07fda9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(68.0, 41.718, 40.0, 24.0)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ppYy8KWAoia1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2okc08kUpbqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('MountainCar-v0')\n",
        "obs = env.reset()"
      ],
      "metadata": {
        "id": "STSEFbgzpe5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.seed(42)\n",
        "\n",
        "def basic_policy(obs):\n",
        "    angle = obs[2]\n",
        "    return 0 if angle < 0 else 1\n",
        "\n",
        "def random_policy(obs):\n",
        "  return random.randint(0,1)\n",
        "\n",
        "totals = []\n",
        "for episode in range(500):\n",
        "    episode_rewards = 0\n",
        "    obs = env.reset()\n",
        "    for step in range(200):\n",
        "        #action = basic_policy(obs)\n",
        "        action = random_policy(obs)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        episode_rewards += reward\n",
        "        if done:\n",
        "            break\n",
        "    totals.append(episode_rewards)"
      ],
      "metadata": {
        "id": "qCbGbIpkq5_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Policy"
      ],
      "metadata": {
        "id": "Gt3hXphnrFYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(totals), np.mean(totals), np.median(totals), np.min(totals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1FG4hxWrC1T",
        "outputId": "a975b1af-473b-43d7-a1ff-210b7d515fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-200.0, -200.0, -200.0, -200.0)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OAFI16FXrE7C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}